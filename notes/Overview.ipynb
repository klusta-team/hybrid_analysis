{
 "metadata": {
  "name": "Overview"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Original data files\n",
      "There is a folder consisting of the three recordings (.dat files) of Mariano Belluscio. One .dat file has been clustered (`n6mab031109.dat`),\n",
      "with method `MKKdistfloat` and has a number of nice clusters out of which we can choose a few suitable donor cells. There are two other recordings\n",
      "`n6mab041109.dat` and `n6mab061109.dat`; these will act as \"acceptor\" datasets.\n",
      "\n",
      "#Hybrid dataset creation\n",
      "\n",
      "Minimal information necessary for defining a hybrid dataset is a 4-tuple of the form: $\\mbox{(acceptor, donor_id, amplitude, time_series)}$,\n",
      "    where `acceptor` is the dataset which receives spikes from another analogously recorded dataset `donor` at the times specified by `time_series`.\n",
      "Each element of the 4-tuple may be generated by a range of other parameters, e.g.\n",
      "\n",
      "1) A rate, $r$ could define a `time_series` consisting of regular spiking at $r$ Hz.\n",
      "\n",
      "2) The donor_id could be derived from information, e.g. the 3-tuple $\\mbox{(donor,donorcluid,donorcluster)}$ pertaining to the donor dataset (donor),\n",
      "the detection method and clustering algorithm (donorcluid) and cluster (donorcluster) number of the cell that is added to the acceptor dataset.\n",
      "\n",
      "    def: create_donor_id(donor,donorcluid,donorcluster):\n",
      "    \n",
      "    output: \n",
      "    donor_id = donor_donorcluid_donorcluster, (which typically looks like: `n6mab031109_MKKdistfloat_54`).\n",
      "    meanspike_file_id = a file called donor_id.msua.1\n",
      "    meanmask_file = a file called donor_id.amsk.1\n",
      "    \n",
      "We can have a folder containing donor_ids, or a list of 3-tuples generating to donor_ids.   \n",
      "    \n",
      "We can create a hybrid dataset $D_{\\mbox{(acceptor, donor_id, amplitude, time_series)}}$ (which we shall abbreviate to $D$ when there is\n",
      " no ambiguity) using the Python function:\n",
      "    def: create_hybrid_datfile(acceptor, donor_id, amplitude, time_series):\n",
      "        \n",
      "This function outputs a raw datafile called, $\\mathcal{H}_{D}$.dat, where $\\mathcal{H}_{\\mbox{var}}$ is the hash produced from the variables `var`\n",
      "via some hash function.     \n",
      "    \n",
      "Question: The raw waveform from one recording on non-relevant channels could contain wildly inappropriate waveforms. I got around this before\n",
      "by only added parts of the wave form corresponding to the .amsk.1 file.    \n",
      "    \n",
      "#Running SpikeDetekt with various parameters\n",
      "\n",
      "We aim to find the optimal detection strategy, but running spikedetekt on $\\mathcal{H}_{D}$.dat  several times using a variety of parameters, `params`,\n",
      "which we shall denote $p$, resulting in two files in the folder\n",
      "$\\mathcal{H}_{D}\\_\\mathcal{H}_{p}$:\n",
      "\n",
      "$\\mathcal{H}_{D}\\_\\mathcal{H}_{p}$.kwx \n",
      "\n",
      "$\\mathcal{H}_{D}\\_\\mathcal{H}_{p}$.kwik\n",
      "\n",
      "The feature and mask vectors, corresponding to the .fet and .fmask files are stored in the .kwx file, these will later be required by Masked KlustaKwik.\n",
      "\n",
      "The parameters are stored in a global Python dictionary of  variables which can be accessed by all modules of SpikeDetekt.\n",
      "The default value of these parameters are stored in `spikedetekt/spikedetekt/defaultparameters.py`. \n",
      "\n",
      "In the first phase we would like to finish testing the effect of varying certain parameters on the quality of spike detection and alignment,\n",
      "e.g. testing two-threshold detection by keeping the upper threshold constant and varying the lower threshold. We will specify a 2D detection\n",
      "window, $\\Sigma$ consisting of a minimal time jitter window and a threshold of minimal mask similarity measure (>0.8); a spike will be denoted\n",
      "as \"successfully detected\" if it lies within this detection window. This will be implemented in some Python functions in `evaluate_detection.py`.\n",
      "This will have two outputs: \n",
      "\n",
      "1) `detection_statistics`$(D, p,\\Sigma)$ will be a set of scripts which will measure the efficacy of the detection $p$.\n",
      "\n",
      "2) A `derived groundtruth`$(D, p,\\Sigma)$ relative to the detection (i.e. equivalent to a .clu file which is commensurate with the output .res file\n",
      " of detected spikes(those found in the window specified by $\\Sigma$, which specifies which spikes are background (in cluster 0) and which are hybrid\n",
      " (in clusters $1, 2,\\ldots, \\mbox{num_hybrids}$. This clustering can be stored in $(D, p,\\Sigma)$.kwik.\n",
      "  \n",
      "#Supervised Learning\n",
      "To perform supervised learning in the form of a support vector machine (SVM) on the groundtruth obtained after running SpikeDetekt\n",
      "and applying detection criterion $\\Sigma$, we use the Python machine learning package, `sklearn`. We will run SVM\n",
      "with a different kernels and their associated parameters, and a grid of class weights. We shall denote these parameters, $\\sigma$.\n",
      "The set of functions `supervised_learning` should output a set of clusterings from which we obtain an ROC curve, providing an \n",
      "upper bound for performance of any unsupervised algorithm. This set of clusterings will be stored in the file $(D, p,\\Sigma,\\sigma)$.kwik.\n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "#Clustering using Masked KlustaKwik\n",
      "Given detection, extraction of features and masking, specified in the files $\\mathcal{H}_{D}\\_\\mathcal{H}_{p}$.kwx, we can now run\n",
      "KlustaKwik with a set of parameters which we shall denote $k$, on the .fet and .fmask files obtained from $\\mathcal{H}_{D}\\_\\mathcal{H}_{p}$.kwx.\n",
      " This will result in a clustering which will be stored in the file $\\mathcal{H}_{D}\\_\\mathcal{H}_{p}\\_\\mathcal{H}_{k}$.kwx. An analysis script can \n",
      " be run on $\\mathcal{H}_{D}\\_\\mathcal{H}_{p}\\_\\mathcal{H}_{k}$.kwx and the associated groundtruth, $(D, p,\\Sigma)$.kwik to obtain the \n",
      " confusion matrix. This will be stored as a pickle or a textfile $\\mathcal{H}_{D}\\_\\mathcal{H}_{p}\\_\\mathcal{H}_{k}\\_\\Sigma$.p.\n",
      "                                                                                                                                                                                                                                              M#\n",
      "\n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}