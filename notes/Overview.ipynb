{
 "metadata": {
  "name": "Overview"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Important Preliminaries:\n",
      "1). The .kwik format has to be modified so that it can store more than one clustering.\n",
      "\n",
      "2). Which hashing algorithm? Something from [http://docs.python.org/2/library/hashlib.html](http://docs.python.org/2/library/hashlib.html). \n",
      "We obviously don't need cryptography. \n",
      "\n",
      "Warning: Hashing dictionaries can cause problems. Hash a dictionary by first converting to a\n",
      "list and then sorting by key:\n",
      "    \n",
      "    thelist = thedict.items()\n",
      "    thelist.sort(key=item[0])\n",
      "    hash(thelist)\n",
      "    \n",
      "or\n",
      "\n",
      "    hash(sorted(thedict.items(), key=lambda (key, val): key))\n",
      "\n",
      "3) Using joblib [http://pythonhosted.org/joblib/](http://pythonhosted.org/joblib/) seems\n",
      "extremely worthwhile. I will specify in detail how to do this. We shall\n",
      "employ a method of caching and hashing/\n",
      "\n",
      " It works by caching the result of computing a function. It will\n",
      "only compute a function once, the first time you call it; the second time, it will just load the cached\n",
      "value from disk. So if you have a joblib decorated function f, you can use\n",
      "f.get_output_dir(*args, **kwds) to to give the directory where the output files/data were stored. \n",
      "i.e. when you decorate a function, it turns automatically into a member of a class with that attribute.\n",
      "\n",
      "We need to avoid using lambda functions as they need to be picklable.\n",
      "\n",
      "As a result of memoization, we can write every script you want to run as though it has to run from scratch each time, which \n",
      "means not having to worry about managing the saving of data. It manages this for you. \n",
      "\n",
      "4) Caching the hash function of the various input parameters (Python dictionaries), as\n",
      "recommended by Dan.\n",
      "\n",
      "Here is a small useful wrapper for joblib:\n",
      "    \n",
      "    import joblib\n",
      "    \n",
      "    '''Wrapper for joblib. \n",
      "       Define some directory  cache_path somewhere.\n",
      "       Use @func_cache to cache a function.\n",
      "       Use cache_hash(f, *args, **kwds) to get the hash of the arguments and keywords as a string\n",
      "\n",
      "       MAIN IDEA: Use this string as a directory name because it is reproducible on\n",
      "       any Python 64-bit distribution. Exploit this reproducibility to \n",
      "       name any directory structures. '''\n",
      "    cache_mem = joblib.Memory(cachedir=cache_path, verbose=0)\n",
      "\n",
      "    func_cache = cache_mem.cache\n",
      "\n",
      "    def is_cacheable(func):\n",
      "        return hasattr(func, 'get_output_dir')\n",
      "    #i.e. was the function func wrapped with joblib\n",
      "    \n",
      "    def is_cached(func, *args, **kwds):\n",
      "        s = func.get_output_dir(*args, **kwds)\n",
      "        return os.path.exists(func.get_output_dir(*args, **kwds)[0])\n",
      "    #i.e. to check if a particular set of keywords and arguments \n",
      "    #have already been cached\n",
      "\n",
      "    def cache_hash(func, *args, **kwds):\n",
      "        _, hash = func.get_output_dir(*args, **kwds)\n",
      "        return hash\n",
      "    \n",
      "    if __name__=='__main__':\n",
      "        @func_cache\n",
      "        def f(x):\n",
      "            '''Define some function which does something. \n",
      "             It may have side effects'''\n",
      "            return whatever\n",
      "\n",
      "5) Usage of dictionaries as input. \n",
      "Datasets can be described by the tuple of inputs needed to create them. Functions can take\n",
      "dictionaries as input. \n",
      "\n",
      "        \n",
      "#Original data files\n",
      "There is a folder consisting of the three recordings (.dat files) of Mariano Belluscio.\n",
      "One .dat file has been clustered (`n6mab031109.dat`), with method `MKKdistfloat` and has\n",
      "a number of nice clusters out of which we can choose a few suitable donor cells. There\n",
      "are two other recordings `n6mab041109.dat` and `n6mab061109.dat`; these will act as \n",
      "\"acceptor\" datasets.\n",
      "\n",
      "\n",
      "#Hybrid dataset creation\n",
      "\n",
      "Minimal information necessary for defining a hybrid dataset is a 4-tuple of the form:\n",
      "`(acceptor, donor_id, amplitude, time_series)`,\n",
      "where `acceptor` is the dataset which receives spikes from another analogously recorded\n",
      "dataset `donor` at the times specified by `time_series`.\n",
      "Each element of the 4-tuple may be generated by a range of other parameters, e.g.\n",
      "\n",
      "1) A rate, `r` could define a `time_series` consisting of regular spiking at `r` Hz.\n",
      "This could be created by a function:\n",
      "    \n",
      "    def: create_regular_resfile(rate, sampling_rate, starttime, endtime, resname):\n",
      "            '''Creates a regular .res file with firing rate r Hz\n",
      "          samples'''\n",
      "\n",
      "2) The donor_id could be derived from information, e.g. the 3-tuple \n",
      "(donor,donorcluid,donorcluster) pertaining to the donor dataset (donor),\n",
      "the detection method and clustering algorithm (donorcluid) and cluster (donorcluster)\n",
      "number of the cell that is added to the acceptor dataset.\n",
      "\n",
      "    @func_cache\n",
      "    def create_donor_id(donor,donorcluid,donorcluster):\n",
      "    '''Outputs donor identity files: \n",
      "\n",
      "    donor_id = donor_donorcluid_donorcluster, (which typically looks like: n6mab031109_MKKdistfloat_54`).\n",
      "\n",
      "    meanspike_file_id = a file called donor_id.msua.1.\n",
      "\n",
      "    meanmask_file = a file called donor_id.amsk.1.\n",
      "    \n",
      "    It will return a dictionary: \n",
      "    donor_dict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54 }\n",
      "\n",
      "    This function should be decorated. '''\n",
      "        return donor_id, donor_dict\n",
      "    \n",
      "Question: For consistency we could have the donor_ids as hashes. However since there are going to be so few\n",
      "why not keep the directory name human readable?\n",
      "    \n",
      "We can have a folder containing donor_ids, or a list of 3-tuples generating to donor_ids.   \n",
      "    \n",
      "We can create a hybrid dataset `D(acceptor, donor_id, amplitude, time_series)`\n",
      "(which we shall abbreviate to `D` when there is no ambiguity) using the Python function:\n",
      "    \n",
      "    hybdatarglist = [acceptor, donor_id, amplitude, time_series]\n",
      "    \n",
      "    @func_cache\n",
      "    def create_hybdatfile_directory_name(**hybdatadict):\n",
      "        return (donor_id, amplitude, time_series)\n",
      "    \n",
      "    cache_hash(create_directory_name, **hybdatadict)\n",
      "    #This will give you Hash(D)\n",
      "    \n",
      "Hence within any function that requires Hash(D), simply include the lines:\n",
      "    \n",
      "    if is_cached(create_hybdatfile_directory_name, **hybdatdict):\n",
      "        dirname = cache_hash(create_hybdatfile_directory_name, **hybdatdict)\n",
      "      \n",
      "    \n",
      "    def create_hybrid_datfile(**hybdatdict):\n",
      "        \n",
      "    '''This function outputs a raw datafile called, \n",
      "          Hash(D).dat, in the folder Hash(D)'''\n",
      "    \n",
      "    def convert_to_kwik():\n",
      "            '''Converts Hash(D).dat to Hash(D).kwd and Hash(D).kwik'''\n",
      "    \n",
      ", where Hash(var) is the hash produced from the variables `var` via some\n",
      "hash function.     \n",
      "    \n",
      "Question: The raw waveform from one recording on non-relevant channels could contain\n",
      "wildly inappropriate waveforms. I got around this before\n",
      "by only added parts of the wave form corresponding to the .amsk.1 file.   \n",
      "\n",
      "NOTE: `D` implicitly contains the groundtruth times (equivalent to a .res and a .clu file) for the\n",
      "hybrid spike times and hybrid clusters.\n",
      "        \n",
      "    \n",
      "#Running SpikeDetekt with various parameters\n",
      "\n",
      "We aim to find the optimal detection strategy, but running spikedetekt on Hash(D).dat\n",
      "several times using a variety of parameters, `params`,\n",
      "which we shall denote `p`, resulting in two files in the folder\n",
      "\n",
      "    @func_cache\n",
      "        def create_paramsfile_directory_name(*p):\n",
      "        '''Will create the hash for p'''\n",
      "            return (p)\n",
      "\n",
      "`Hash(D)_Hash(p)`:\n",
      "    \n",
      "`Hash(D)_Hash(p).kwx`\n",
      "    \n",
      "`Hash(D)_Hash(p).kwik`\n",
      "    \n",
      "\n",
      "The feature and mask vectors, corresponding to the .fet and .fmask files are stored in the .kwx file,\n",
      "these will later be required by Masked KlustaKwik.\n",
      "\n",
      "The parameters are stored in a global Python dictionary of  variables which can be accessed by all modules of SpikeDetekt.\n",
      "The default value of these parameters are stored in `spikedetekt/spikedetekt/defaultparameters.py`. \n",
      "\n",
      "In the first phase we would like to finish testing the effect of varying certain parameters on the quality of spike detection and alignment,\n",
      "e.g. testing two-threshold detection by keeping the upper threshold constant and varying the lower threshold.\n",
      "We can vary one parameter and keep the others constant by specifying custom default parameters in the file `usualparameters.py`\n",
      ".  We will specify a 2D detection\n",
      "window, `Sigma` consisting of a minimal time jitter window and a threshold of minimal mask similarity measure (>0.8); a spike will be denoted\n",
      "as \"successfully detected\" if it lies within this detection window. This will be implemented in some Python functions in `evaluate_detection.py`.\n",
      "This will have two outputs: \n",
      "\n",
      "1) `detection_statistics(D, p,Sigma)` will be a set of scripts which will measure the efficacy of the detection `p`.\n",
      "e.g. we could have a function:\n",
      "    def test_detection_algorithm(D,p,Sigma):\n",
      "            '''Test spike detection algorithm on a hybrid dataset'''\n",
      "\n",
      "2) A `derived groundtruth(D, p,Sigma)` relative to the detection (i.e. equivalent to a .clu file which is\n",
      " commensurate with the output .res file of detected spikes(those found in the window specified by `Sigma`,\n",
      " which specifies which spikes are background (in cluster 0) and which are hybrid\n",
      " (in clusters 1, 2, ... num_hybrids. This clustering can be stored in Hash(D)_Hash(p)_Sigma.kwik.\n",
      "  \n",
      "#Supervised Learning\n",
      "To perform supervised learning in the form of a support vector machine (SVM) on the groundtruth obtained after running SpikeDetekt\n",
      "and applying detection criterion `Sigma`, we use the Python machine learning package, `sklearn`. We will run SVM\n",
      "with a different kernels and their associated parameters, and a grid of class weights. We shall denote these parameters, s.\n",
      "The set of functions `supervised_learning` should output a set of clusterings from which we obtain an ROC curve, providing an \n",
      "upper bound for performance of any unsupervised algorithm. This set of clusterings will be stored in the file `(D, p,Sigma,s)`.kwik.\n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "#Clustering using Masked KlustaKwik\n",
      "Given detection, extraction of features and masking, specified in the files `Hash(D)_Hash(p).kwx`, we can now run\n",
      "KlustaKwik on the .fet and .fmask files obtained from `Hash(D)_Hash(p).kwx`. with a set of parameters which we shall\n",
      " denote `k` which will be defined a dictionary which will\n",
      "output a bash script for running KlustaKwik, \n",
      "\n",
      "    def get_KK_input(D,p,k):\n",
      "    '''Will output the .fet and .fmask files in a folder, CartesianHash_(D,p,k)'''\n",
      "    \n",
      "    def make_KK_runscripts(D,p,k):\n",
      "    '''Produces bash scripts pertaining to parameters k that run KlustaKwik on (D,p) '''\n",
      "    \n",
      "    def make_bash_hash(D,p,k):\n",
      "    '''Outputs the hash of (D,p,k) and the dictionary k'''\n",
      "                                                                                                                                                                                                                                              \n",
      "                                                                                                                                                                                                                                              \n",
      "This will result in a clustering which will be stored in the file `Hash(D)_Hash(p)_Hash(k).kwx`. An analysis script can \n",
      "be run on `Hash(D)_Hash(p)_Hash(k).kwx` and the associated groundtruth, `Hash(D)_Hash(p)_Sigma.kwik` to obtain the \n",
      "confusion matrix. This will be stored as a pickle or a textfile `Hash(D)_Hash(p)_Hash(k).p`.\n",
      "                                                                                                                                                                                                                                              M#\n",
      "#Supercomputer\n",
      "Scripts will be required for sending jobs to Legion. These jobs will usually be SVM and Masked KlustaKwik.\n",
      "\n",
      "#Folder structure\n",
      "In summary the folder structure will look as follows:\n",
      "\n",
      "    /Testsuite\n",
      "        listoftuples.py\n",
      "        masterscript.py\n",
      "        hybridata_creation_lib.py\n",
      "        runspikedetekt_lib.py\n",
      "        runklustakwik_lib.py\n",
      "        runsupervised_lib.py\n",
      "        /Mariano\n",
      "            /donors\n",
      "            /acceptors  \n",
      "        /Hybrids\n",
      "            /Hash(D)\n",
      "            /Hash(D)_Hash(p)\n",
      "            /Hash(D)_Hash(p)_Hash(k)\n",
      "            \n",
      "            \n",
      "Each script will contain the following functions:\n",
      "    listoftuples.py:\n",
      "        donorlist = [donor,donorcluid,donorcluster]\n",
      "        acceptorlist = [acceptor]\n",
      "        time_size_list = [amplitude, time_series]\n",
      "        #hybdatarglist = [acceptor, donor_id, amplitude, time_series]\n",
      "    \n",
      "    input_generator.py: (will output to listoftuples.py)\n",
      "        def convert_tuple_to_dict(*arglist):\n",
      "            \n",
      "        def merge_inputdicts(donordict,acceptordict,time_size_dict): \n",
      "            return hybdatadict\n",
      "            \n",
      "        \n",
      "        \n",
      "    hybridata_creation.py:\n",
      "        def create_donor_id(): \n",
      "        \n",
      "        def create_hybdatfile_directory_name(): \n",
      "        def create_hybrid_datfile(acceptor, donor_id, amplitude, time_series): \n",
      "        \n",
      "        \n",
      "            \n",
      "            \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}