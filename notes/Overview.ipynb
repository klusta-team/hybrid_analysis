{
 "metadata": {
  "name": "Overview"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Important Preliminaries and Principles:\n",
      "1). The .kwik format has to be modified so that it can store more than one clustering.\n",
      "\n",
      "2). Which hashing algorithm? Something from [http://docs.python.org/2/library/hashlib.html](http://docs.python.org/2/library/hashlib.html). \n",
      "We obviously don't need cryptography. \n",
      "\n",
      "Warning: Hashing dictionaries can cause problems. Hash a dictionary by first converting to a\n",
      "list and then sorting by key:\n",
      "    \n",
      "    thelist = thedict.items()\n",
      "    thelist.sort(key=item[0])\n",
      "    hash(thelist)\n",
      "    \n",
      "or\n",
      "\n",
      "    hash(sorted(thedict.items(), key=lambda (key, val): key))\n",
      "    \n",
      "However we DO NOT need to worry about this, because we can used joblib (see below).\n",
      "\n",
      "3) Using joblib [http://pythonhosted.org/joblib/](http://pythonhosted.org/joblib/) seems\n",
      "extremely worthwhile. I will specify in detail how to do this. We shall\n",
      "employ a method of caching and hashing and we avoid the dictionary hashing problem.\n",
      "\n",
      " It works by caching the result of computing a function. It will\n",
      "only compute a function once, the first time you call it; the second time, it will just load the cached\n",
      "value from disk. So if you have a joblib decorated function f, you can use\n",
      "f.get_output_dir(*args, **kwds) to to give the directory where the output files/data were stored. \n",
      "i.e. when you decorate a function, it turns automatically into a member of a class with that attribute.\n",
      "\n",
      "We need to avoid using lambda functions as they need to be picklable.\n",
      "\n",
      "As a result of memoization, we can write every script you want to run as though it has to run from scratch each time, which \n",
      "means not having to worry about managing the saving of data. It manages this for you. \n",
      "\n",
      "4) Caching the hash function of the various input parameters (Python dictionaries), as\n",
      "recommended by Dan.\n",
      "\n",
      "Here is a small useful wrapper for joblib:\n",
      "    \n",
      "    ``import joblib\n",
      "    \n",
      "    '''Wrapper for joblib. \n",
      "       Define some directory  cache_path somewhere.\n",
      "       Use @func_cache to cache a function.\n",
      "       Use cache_hash(f, *args, **kwds) to get the hash of the arguments and keywords as a string\n",
      "\n",
      "       MAIN IDEA: Use this string as a directory name because it is reproducible on\n",
      "       any Python 64-bit distribution. Exploit this reproducibility to \n",
      "       name any directory structures. '''\n",
      "    cache_mem = joblib.Memory(cachedir=cache_path, verbose=0)\n",
      "\n",
      "    func_cache = cache_mem.cache\n",
      "\n",
      "    def is_cacheable(func):\n",
      "        return hasattr(func, 'get_output_dir')\n",
      "    #i.e. was the function func wrapped with joblib\n",
      "    \n",
      "    def is_cached(func, *args, **kwds):\n",
      "        s = func.get_output_dir(*args, **kwds)\n",
      "        return os.path.exists(func.get_output_dir(*args, **kwds)[0])\n",
      "    #i.e. to check if a particular set of keywords and arguments \n",
      "    #have already been cached\n",
      "\n",
      "    def cache_hash(func, *args, **kwds):\n",
      "        _, hash = func.get_output_dir(*args, **kwds)\n",
      "        return hash\n",
      "    \n",
      "    if __name__=='__main__':\n",
      "        @func_cache\n",
      "        def f(x):\n",
      "            '''Define some function which does something. \n",
      "             It may have side effects'''\n",
      "            return whatever\n",
      "      ``\n",
      "        \n",
      "        \n",
      "5) Functions written for the purposes of analysis will be pure and be cached with joblib.\n",
      "This will avoid confusion as to whether or not something has been analysed already and\n",
      "the results of any analysis can simply be loaded from the cached value. Once loaded\n",
      "any visualizations derived from the analysis can be produced using an IPython notebook.\n",
      "\n",
      "6)Functions with side effects, e.g. running SpikeDetekt, KlustaKwik, etc. will only\n",
      "have the names of directories containing their results cached using joblib. \n",
      "Analysis functions will be written so that we can iterate over the various input\n",
      "dictionaries and but will find the appropriate files according to the directory\n",
      "name specified by the function `cache_hash(func, **inputdict)`.\n",
      "                \n",
      "7) Usage of dictionaries as input. \n",
      "Datasets can be described by the tuple of inputs needed to create them. Functions can take\n",
      "dictionaries as input. \n",
      "\n",
      "        \n",
      "#Original data files\n",
      "There is a folder consisting of the three recordings (.dat files) of Mariano Belluscio.\n",
      "One .dat file has been clustered (`n6mab031109.dat`), with method `MKKdistfloat` and has\n",
      "a number of nice clusters out of which we can choose a few suitable donor cells. There\n",
      "are two other recordings `n6mab041109.dat` and `n6mab061109.dat`; these will act as \n",
      "\"acceptor\" datasets.\n",
      "\n",
      "\n",
      "#Hybrid dataset creation\n",
      "\n",
      "Minimal information necessary for defining a hybrid dataset is a 4-tuple of the form:\n",
      "`(acceptor, donor_id, amplitude, time_series)`,\n",
      "where `acceptor` is the dataset which receives spikes from another analogously recorded\n",
      "dataset `donor` at the times specified by `time_series`.\n",
      "Each element of the 4-tuple may be generated by a range of other parameters, e.g.\n",
      "\n",
      "1) A rate, `r` could define a `time_series` consisting of regular spiking at `r` Hz.\n",
      "This could be created by a function:\n",
      "    \n",
      "    def: create_regular_resfile(rate, sampling_rate, starttime, endtime, resname):\n",
      "            '''Creates a regular .res file with firing rate r Hz\n",
      "          samples'''\n",
      "\n",
      "2) The donor_id could be derived from information, e.g. the 3-tuple \n",
      "(donor,donorcluid,donorcluster) pertaining to the donor dataset (donor),\n",
      "the detection method and clustering algorithm (donorcluid) and cluster (donorcluster)\n",
      "number of the cell that is added to the acceptor dataset.\n",
      "\n",
      "      ``@func_cache\n",
      "      def create_donor_id(donor,donorcluid,donorcluster):\n",
      "      '''Outputs donor identity files: \n",
      "\n",
      "      donor_id = donor_donorcluid_donorcluster, (which typically looks like: n6mab031109_MKKdistfloat_54`).\n",
      "\n",
      "      meanspike_file_id = a file called donor_id.msua.1.\n",
      "\n",
      "      meanmask_file = a file called donor_id.amsk.1.\n",
      "    \n",
      "      It will return a dictionary: \n",
      "      donor_dict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54 }\n",
      "\n",
      "      This function should be decorated. '''\n",
      "          return donor_id, donor_dict``\n",
      "    \n",
      "Question: For consistency we could have the donor_ids as hashes. However since there are going to be so few\n",
      "why not keep the directory name human readable?\n",
      "    \n",
      "We can have a folder containing donor_ids, or a list of 3-tuples generating to donor_ids.   \n",
      "    \n",
      "We can create a hybrid dataset `D(acceptor, donor_id, amplitude, time_series)`\n",
      "(which we shall abbreviate to `D` when there is no ambiguity) using the Python function:\n",
      "    \n",
      "    hybdatarglist = [acceptor, donor_id, amplitude, time_series]\n",
      "    #This could all get converted to e.g. \n",
      "    hybdatadict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54,\n",
      "                   'acceptor':'n6mab041109quarter1', 'amplitude':0.5,\n",
      "                   'donorspike_timeseries': 'regular2p5'}\n",
      "    \n",
      "    \n",
      "    @func_cache\n",
      "    def create_hybdatfile_directory_name(**hybdatadict):\n",
      "        hash(sorted(hybdatadict.items(), key=lambda (key, val): key))\n",
      "        return (donor_id, amplitude, time_series)\n",
      "    \n",
      "    #Invoke:\n",
      "    cache_hash(create_hybdatfile_directory_name, **hybdatadict)\n",
      "    #This will give you Hash(D)~Hash(hybdatadict)\n",
      "    \n",
      "Hence within any function that requires Hash(D), simply include the lines:\n",
      "    \n",
      "    if is_cached(create_hybdatfile_directory_name, **hybdatadict):\n",
      "        dirname = cache_hash(create_hybdatfile_directory_name, **hybdatadict)\n",
      "      \n",
      "    \n",
      "    def create_hybrid_datfile(**hybdatadict):\n",
      "        \n",
      "    '''This function outputs a raw datafile called, \n",
      "          Hash(D).dat, in the folder Hash(D)'''\n",
      "    \n",
      "    def convert_to_kwik():\n",
      "            '''Converts Hash(D).dat to Hash(D).kwd and Hash(D).kwik'''\n",
      "    \n",
      ", where Hash(var) is the hash produced from the variables `var` via some\n",
      "hash function.     \n",
      "    \n",
      "Question: The raw waveform from one recording on non-relevant channels could contain\n",
      "wildly inappropriate waveforms. I got around this before\n",
      "by only added parts of the wave form corresponding to the .amsk.1 file.   \n",
      "\n",
      "NOTE: `D` implicitly contains the groundtruth times (equivalent to a .res and a .clu file) for the\n",
      "hybrid spike times and hybrid clusters.\n",
      "        \n",
      "    \n",
      "#Running SpikeDetekt with various parameters\n",
      "\n",
      "We aim to find the optimal detection strategy, but running spikedetekt on Hash(D).dat\n",
      "several times using a variety of parameters, `params`,\n",
      "which we shall denote `p`, resulting in two files in the folder\n",
      "\n",
      "    @func_cache\n",
      "        def create_paramsfile_directory_name(*p):\n",
      "        '''Will create the hash for p'''\n",
      "            return (p)\n",
      "\n",
      "`Hash(D)_Hash(p)`:\n",
      "    \n",
      "`Hash(D)_Hash(p).kwx`\n",
      "    \n",
      "`Hash(D)_Hash(p).kwik`\n",
      "    \n",
      "\n",
      "The feature and mask vectors, corresponding to the .fet and .fmask files are stored in the .kwx file,\n",
      "these will later be required by Masked KlustaKwik.\n",
      "\n",
      "The parameters are stored in a global Python dictionary of  variables which can be accessed by all modules of SpikeDetekt.\n",
      "The default value of these parameters are stored in `spikedetekt/spikedetekt/defaultparameters.py`. \n",
      "\n",
      "In the first phase we would like to finish testing the effect of varying certain parameters on the quality of spike detection and alignment,\n",
      "e.g. testing two-threshold detection by keeping the upper threshold constant and varying the lower threshold.\n",
      "We can vary one parameter and keep the others constant by specifying custom default parameters in the file `usualparameters.py`\n",
      ".  We will specify a 2D detection\n",
      "window, `Sigma` consisting of a minimal time jitter window and a threshold of minimal mask similarity measure (>0.8); a spike will be denoted\n",
      "as \"successfully detected\" if it lies within this detection window. This will be implemented in some Python functions in `evaluate_detection.py`.\n",
      "This will have two outputs: \n",
      "\n",
      "1) `detection_statistics(D, p,Sigma)` will be a set of scripts which will measure the efficacy of the detection `p`.\n",
      "e.g. we could have a function:\n",
      "    def test_detection_algorithm(D,p,Sigma):\n",
      "            '''Test spike detection algorithm on a hybrid dataset'''\n",
      "\n",
      "2) A `derived groundtruth(D, p,Sigma)` relative to the detection (i.e. equivalent to a .clu file which is\n",
      " commensurate with the output .res file of detected spikes(those found in the window specified by `Sigma`,\n",
      " which specifies which spikes are background (in cluster 0) and which are hybrid\n",
      " (in clusters 1, 2, ... num_hybrids. This clustering can be stored in Hash(D)_Hash(p)_Sigma.kwik.\n",
      "  \n",
      "#Supervised Learning\n",
      "To perform supervised learning in the form of a support vector machine (SVM) on the groundtruth obtained after running SpikeDetekt\n",
      "and applying detection criterion `Sigma`, we use the Python machine learning package, `sklearn`. We will run SVM\n",
      "with a different kernels and their associated parameters, and a grid of class weights. We shall denote these parameters, s.\n",
      "The set of functions `supervised_learning` should output a set of clusterings from which we obtain an ROC curve, providing an \n",
      "upper bound for performance of any unsupervised algorithm. This set of clusterings will be stored in the file `(D, p,Sigma,s)`.kwik.\n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "#Clustering using Masked KlustaKwik\n",
      "Given detection, extraction of features and masking, specified in the files `Hash(D)_Hash(p).kwx`, we can now run\n",
      "KlustaKwik on the .fet and .fmask files obtained from `Hash(D)_Hash(p).kwx`. with a set of parameters which we shall\n",
      " denote `k` which will be defined a dictionary which will\n",
      "output a bash script for running KlustaKwik, \n",
      "\n",
      "    @func_cache\n",
      "    def get_KK_input(D,p,k):\n",
      "    '''Will output the .fet and .fmask files in a folder, CartesianHash_(D,p,k)'''\n",
      "    \n",
      "    def make_KK_runscripts(D,p,k):\n",
      "    '''Produces bash scripts pertaining to parameters k that run KlustaKwik on (D,p) '''\n",
      "    \n",
      "    #def make_bash_hash(D,p,k):\n",
      "    #'''Outputs the hash of (D,p,k) and the dictionary k'''\n",
      "    #This is no longer needed. We just take cache the function `get_KK_input`\n",
      "    \n",
      "                                                                                                                                                                                                                                              \n",
      "                                                                                                                                                                                                                                              \n",
      "This will result in a clustering which will be stored in the file `Hash(D)_Hash(p)_Hash(k).kwx`. An analysis script can \n",
      "be run on `Hash(D)_Hash(p)_Hash(k).kwx` and the associated groundtruth, `Hash(D)_Hash(p)_Sigma.kwik` to obtain the \n",
      "confusion matrix. This will be stored as a pickle or a textfile `Hash(D)_Hash(p)_Hash(k)_Sigma.p`.\n",
      "                                                                                                                                                                                                                                              M#\n",
      "#Supercomputer\n",
      "Scripts will be required for sending jobs to Legion. These jobs will usually be SVM and Masked KlustaKwik.\n",
      "\n",
      "#Folder structure\n",
      "In summary the folder structure will look as follows:\n",
      "\n",
      "    /Testsuite\n",
      "        listoftuples.py\n",
      "        masterscript.py\n",
      "        hybridata_creation_lib.py\n",
      "        runspikedetekt_lib.py\n",
      "        runklustakwik_lib.py\n",
      "        runsupervised_lib.py\n",
      "        detection_statistics.py\n",
      "        clustering_performance.py\n",
      "        \n",
      "        \n",
      "        /Mariano\n",
      "            /donors\n",
      "            /acceptors \n",
      " \n",
      "        /Hybrids\n",
      "        \n",
      "            /Hash(D)\n",
      "                Hash(D).kwd\n",
      "                \n",
      "            /Hash(D)_Hash(p)\n",
      "                Hash(D)_Hash(p).kwx\n",
      "                Hash(D)_Hash(p).kwik\n",
      "                \n",
      "            /Hash(D)_Hash(p)_Sigma\n",
      "                Hash(D)_Hash(p)_Sigma.kwik\n",
      "                \n",
      "            /Hash(D)_Hash(p)_Hash(k)\n",
      "        /Analysis\n",
      "            analysis_cache\n",
      "    \n",
      "            \n",
      "            \n",
      "Each script will contain the following functions:\n",
      "\n",
      "        \n",
      "    listoftuples.py:\n",
      "        donorlist = [donor,donorcluid,donorcluster]\n",
      "        acceptorlist = [acceptor]\n",
      "        time_size_list = [amplitude, time_series]\n",
      "        #hybdatarglist = [acceptor, donor_id, amplitude, time_series]\n",
      "    \n",
      "    input_generator.py: (will output to listoftuples.py)\n",
      "        def convert_tuple_to_dict(*arglist):\n",
      "            \n",
      "        def merge_inputdicts(donordict,acceptordict,time_size_dict): \n",
      "            return hybdatadict\n",
      "        def create_average_hybrid_wave_spike():\n",
      "        def make_average datamask_from_mean():\n",
      "        \n",
      "        \n",
      "    hybridata_creation.py:\n",
      "        def create_donor_id(): \n",
      "        \n",
      "        def create_hybdatfile_directory_name(): \n",
      "        def create_hybrid_datfile(**hybdatadict): \n",
      "    runspikedetekt_lib.py:\n",
      "        def concatenate_dict():\n",
      "        def run_SD_oneparamfamily(*param2vary,**hybdatadict):\n",
      "        ''' Parameter to vary and hybrid dataset.\n",
      "         **param2vary is a list of the form\n",
      "            this will loop\n",
      "        through a one parameter family and call run_spikedetekt()'''\n",
      "        def run_spikedetekt(**hybdatadict, **prms): \n",
      "        #OK, I can't realise use this notation, so I suppose I have to produce one dictionary\n",
      "        # containing both the SD parameters and the hybrid dataset parameters\n",
      "    runklustakwik_lib.py:\n",
      "        def make_KK_inputdict():\n",
      "        def get_KK_input(D,p,k):\n",
      "        def run_klustakwik(**hybdatadict,**prms,**k):\n",
      "    runsupervised_lib.py:\n",
      "        def write_kwik():\n",
      "        '''writes cluster assignment output that can be read by klustaviewa\n",
      "            e.g. equivalent to 4 seasons .clu file'''\n",
      "        def select_subsetfeatures():\n",
      "        ''' If you want to only perform supervised learning on a subset of features'''\n",
      "        def scale_data():\n",
      "        ''' Scales data between 0 and 1. Can use sklearn.preprocessing.scale''' \n",
      "        def compute_grid_weights():\n",
      "        def do_cross_validation_every(k_times):                                                                                                                                                                                                   '''\n",
      "        ''subdivide dataset into k_times parts'''\n",
      "        def learn_data(C,kernel, weights, groundtruth, data):\n",
      "        '''utilises sklearn.svm.SVC '''\n",
      "            return confusion_test, confusion_train\n",
      "        def compute_errors(confusion_test,confusion_train):\n",
      "        '''computes errors from confusion matrices'''\n",
      "            return .\n",
      "    detection_statistics.py:\n",
      "        def test_detection_algorithm(**hybdatadict):\n",
      "    clustering_performance.py:\n",
      "        def: get_cache_hash_KK(**hybdatadict):\n",
      "        def test_spike_sorting_algorithm(**hybdatadict,**KKparams, groundtruth):\n",
      "        ''' will call create_confusion_matrix'''\n",
      "        def create_confusion_matrix():\n",
      "        def VImetric():\n",
      "        def EntropyH():\n",
      "        def MutualInf():\n",
      "        def compute_errors(confusion_matrix):\n",
      "    master_analysis_scripts: \n",
      "        #This will make use of functions in clustering_performance.py\n",
      "        #and call upon the directory names vie the function\n",
      "        \n",
      "            \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "                                                                                                                                                                                                                                                         \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}