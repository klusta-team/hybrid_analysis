{
 "metadata": {
  "name": "Detailed_structure"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Folder structure\n",
      "We should aim as much as we can to exploit the joblib cache and obtain the data we need\n",
      "by just calling the function that initially generated the data. We therefore\n",
      "need to just store lists or dictionaries of input parameters and their associated\n",
      "functions. Ideally we want a system that can be used easily by an amnesiac!\n",
      "\n",
      "In summary the folder structure will look as follows:\n",
      "\n",
      "    /Creation\n",
      "        mastercreationscript.py\n",
      "        # One script that will run though the creation of a set of hybridatasets\n",
      "        \n",
      "        hybridata_creation_lib.py\n",
      "        # Will contain all the functions for creating hybrid datasets\n",
      "        /Mariano\n",
      "            /donors\n",
      "            # Contains donor .dat files \n",
      "            # to a specific acceptor .dat file\n",
      "            \n",
      "            ## (Contains input .res files pertaining to how to add spikes-\n",
      "            # Though this is no longer necessary as we can simply cache this\n",
      "            # information from when we run the function that generates the time series\n",
      "            # at which the hybrids spikes are to be added to the acceptor dataset)\n",
      "            \n",
      "            # It will also produce donor derived files such as: \n",
      "            # meanspike_file_id = a file called donor_id.msua.1.\n",
      "            # meanmask_file = a file called donor_id.amsk.1.\n",
      "            \n",
      "            # The existence of these files will also be deprecated and the information\n",
      "            # can instead be the result of running a few known joblib cached functions\n",
      "            /acceptors \n",
      "            # contains acceptor .dat files\n",
      "    \n",
      "    \n",
      "    /Testsuite\n",
      "        listofcreationdicts.py  \n",
      "        # Here we shall store all the dictionaries\n",
      "        # and (possibly named) tuples that we need to create\n",
      "        # and later access hybrid datasets. \n",
      "        \n",
      "        listofprobes.py \n",
      "        # Give dictionaries pertaining to different probes\n",
      "        # We will likely only use one probe as we are unlikely to try second \n",
      "        # nearest neighbours\n",
      "        # probe = {adjacency graph dictionary}\n",
      "        \n",
      "        listofSDparams.py \n",
      "        # Here we need a file containing many different tuples and dictionaries\n",
      "        # all pertaining to different ways of running SpikeDetekt.\n",
      "        # It should certainly contain all the defaultSDparams, which will\n",
      "        # remain unchanged whilst I run\n",
      "        # SpikeDetekt several times varying one parameter using the function\n",
      "        # run_SD_oneparamfamily(*param2vary,**defaultSDparams, **hybdatadict)\n",
      "        \n",
      "        listofMKKparams.py\n",
      "        # Here we shall have a file storing dictionaries which have key, value\n",
      "        # pairs which specify the parameters with which to run KlustaKwik\n",
      "        \n",
      "        hash_utils.py\n",
      "        # Will contain functions useful for hashing. All decorated with joblib.\n",
      "        \n",
      "        runspikedetekt_lib.py\n",
      "        # Will contain all functions for running spikedetekt on hybrid datasets\n",
      "        \n",
      "        detection_statistics.py\n",
      "        # Will contain all functions for analysing the performance of spikedetekt\n",
      "        # and for producing the groundtruth relative to which the performance \n",
      "        # of unsupervised spike sorting can be tested.\n",
      "        \n",
      "        runklustakwik_lib.py\n",
      "        # Will contain all functions for running KlustaKwik given a\n",
      "        # hybrid dataset, a detection with groundtruth by spikedetekt\n",
      "        # It will use the values stored in listofMKKparams.py\n",
      "        \n",
      "        runsupervised_lib.py\n",
      "        # Will contain all functions for running sklearn SVM given a\n",
      "        # hybrid dataset and a detection with groundtruth by spikedetekt\n",
      "        # We will compare two kernels: quadric and Gaussian radial \n",
      "        # basis kernel.\n",
      "        \n",
      "        clustering_performance.py\n",
      "        # Will contain all functions necessary for evaluating the performance of \n",
      "        # KlustaKwik and SVM. All of these will be decorated with joblib.\n",
      "        # Graphs can be plotted with matplotlib - .e.g. ROC curves.\n",
      "        \n",
      " \n",
      "        /Hybrids\n",
      "        # Output files containing data to be used by visualization software:\n",
      "            Hash(hybdatadict).kwd\n",
      "            Hash(hybdatadict)_Hash(sdparams).kwx\n",
      "            Hash(hybdatadict)_Hash(sdparams).kwik  \n",
      "            Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth).kwx\n",
      "            Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth).kwik   \n",
      "            # Hash(detcrit_groundtruth) indicates that the quality of any clusterings should be evaluated \n",
      "            # wrt the groundtruth obtained by detcrit - some spike detection\n",
      "            # quality criteria\n",
      "            Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_KK_Hash(kkparams).kwik\n",
      "            Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_SVM_Hash(svmparams).kwik\n",
      "        /Analysis\n",
      "            analysis_cache\n",
      "            \n",
      "#Detailed specification of files, functions and classes\n",
      "            \n",
      "Each script will contain the following functions and classes (when no\n",
      "return value is specified, the function will be entirely impure):\n",
      "    \n",
      "    listofcreationdicts.py:\n",
      "        donorlist = [(donor,donorcluid,donorcluster)]\n",
      "        acceptorlist = [acceptor]\n",
      "        # where\n",
      "        time_size_dict = {'amplitude':0.5,'donorspike_timeseries_generating_function':\n",
      "                   'create_time_series_constant',\n",
      "                   'donorspike_timeseries_arguments': generator_arg_dictionary}\n",
      "        # the function and arguments needed to create the time series\n",
      "        # note that this will exclude the acceptor although the time series generating function\n",
      "        # itself will require tha acceptor\n",
      "           \n",
      "        hybdatadict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54,\n",
      "                   'acceptor':'n6mab041109quarter1', 'amplitude':0.5, \n",
      "                   'donorspike_timeseries_generating_function':'create_time_series_constant',\n",
      "                   'donorspike_timeseries_arguments': generator_arg_dictionary}\n",
      "        # where donorspike_timeseries_arguments is a dictionary of arguments for the \n",
      "        # donorspike_timeseries_generating_function\n",
      "        # this replaces a label name string, such as 'timeseries:'regular2p5''\n",
      "        \n",
      "    listofprobes.py:\n",
      "    \n",
      "    listofSDparams.py:  \n",
      "        SDusualparams \n",
      "        # dictionaries of default parameters used for different\n",
      "        # testings of SpikeDetekt, where one particular parameter is varied\n",
      "    \n",
      "    listofKKparams.py: \n",
      "        kkdefaultparams\n",
      "        # Dictionaries of default KK command line options\n",
      "            \n",
      "    hybridata_creation_lib.py:\n",
      "    #A collection of functions for creating hybrid datasets\n",
      "        def convert_donortuple_to_dict(*donorarglist):\n",
      "        ''' Will convert my list of tuples into dictionaries of the same length\n",
      "            adding the appropriate keys, e.g.\n",
      "            donorarglist = [(donor,donorcluid,donorcluster)] will turn into a dictionary like:\n",
      "            donorargdict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54}\n",
      "            (will append output to listofcreationtuples.py)\n",
      "          '''   \n",
      "            return donorargdict\n",
      "        def convert_hybtuple_to_dict(*hybdatatuple)\n",
      "            '''Converts hybdatatuple, the necessary information for labelling a \n",
      "            hybrid dataset to a dictionary, looking like the following:\n",
      "            hybdatadict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54,\n",
      "                   'acceptor':'n6mab041109quarter1', 'amplitude':0.5, \n",
      "            'donorspike_timeseries_generating_function': 'create_time_series_constant',\n",
      "                   'donorspike_timeseries_arguments': generator_dictionary}'''\n",
      "            return hybdatadict\n",
      "            \n",
      "        def merge_inputdicts(donorargdict,acceptordict,time_size_dict): \n",
      "        ''' Will merge all the necessary tuples which point to the files needed to \n",
      "            create a hybrid dataset'''\n",
      "            return hybdatadict    \n",
      "        @func_cache\n",
      "        def create_time_series_constant(r,acceptor, samplerate, start = 0, end = None):\n",
      "            '''Will create time series for constant rate,\n",
      "               this will be cached and stored for future reference \n",
      "             when creating hybrid datasets and for analysis'''\n",
      "            return donorspike_timeseries\n",
      "        \n",
      "        def create_donor_id(): \n",
      "        '''Outputs donor identity files: \n",
      "          donordata = (donor,donorcluid,donorcluster)      \n",
      "          donor_id = donor_donorcluid_donorcluster, (which typically looks like: n6mab031109_MKKdistfloat_54`).\n",
      "          will call function: create_average_hybrid_wave_spike(**donor_dict)\n",
      "          meanspike_file_id = a file called donor_id.msua.1.\n",
      "          Will call make_average datamask_from_mean(**donor_dict)\n",
      "          meanmask_file = a file called donor_id.amsk.1.\n",
      "          Alternatively we can simply store the donor details in cache and access it whenever necessary\n",
      "          It will return a dictionary: \n",
      "           donor_dict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54 }\n",
      "           This function should be decorated. '''\n",
      "            return donor_dict, donor_id.msua_data, donor_id.amsk_data\n",
      "        def create_average_hybrid_wave_spike(**donor_dict):\n",
      "        '''will  create the mean spike file from taking the average of\n",
      "          spikes in a donor cluster of the donor dataset\n",
      "          meanspike_file_id = a file called donor_id.msua.1.'''\n",
      "            return donor_id.msua_data\n",
      "        def make_average datamask_from_mean(**donor_dict):\n",
      "        '''Will create the mean mask file from the donor_dict information\n",
      "          meanmask_file = a file called donor_id.amsk.1.\n",
      "           This will be used when computing the spike similarity measure\n",
      "           to assess the quality of spike detection'''\n",
      "            return donor_id.amsk_data\n",
      "        def create_hybkwdfile_namehash(**hybdatadict): \n",
      "            name_hash = hash(sorted(hybdatadict.items(), key=lambda (key, val): key))\n",
      "            return name_hash\n",
      "        @func_cache\n",
      "        def create_hybrid_kwdfile(hybdatadict):\n",
      "            \n",
      "        '''This function outputs a raw datafile called, \n",
      "              Hash(D).kwd,\n",
      "          It adds a mean waveform to an acceptor .dat file\n",
      "            '''\n",
      "            hashD = create_hybkwdfile_namehash(hybdatadict)\n",
      "            return hashD    \n",
      "    \n",
      "    hash_utils.py\n",
      "        # A collection of auxiliary functions giving hashes and \n",
      "        # product hashes\n",
      "        @func_cache\n",
      "        def hash_dictionary(dict):\n",
      "        #returns hashed dictionary\n",
      "            return hashdict\n",
      "        def get_KK_output_hash(hybdatadict,sdprms,detcrit_groundtruth,kkparams):\n",
      "        '''gives the product hash of the input dictionaries'''\n",
      "            return Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_KK_Hash(kkparams)\n",
      "        def get_SVM_output_hash(hybdatadict,sdprms,detcrit_groundtruth,svmparams):\n",
      "        '''gives the product hash of the input dictionaries'''\n",
      "            return Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_SVM_Hash(svmparams)\n",
      "        def get_product_hash(dictlist):\n",
      "            '''gives the product hash of a list of dictionaries, \n",
      "               e.g. dictlist = [dict1, dict2, dict3]\n",
      "               product_hash = Hash(dict1)_Hash(dict2)_Hash(dict3)'''\n",
      "            return product_hash\n",
      "        \n",
      "        \n",
      "        \n",
      "    runspikedetekt_lib.py:\n",
      "        #A collection of functions for running SpikeDetekt\n",
      "        #All decorated with joblib\n",
      "        import hash_utils.py\n",
      "        @func_cache\n",
      "        def run_spikedetekt(sdparams,hybdatadict):\n",
      "        '''This function will call hash_hyb_SD(sdparams,hybdatadict) \n",
      "            and will run SpikeDetekt on the hybrid dataset specified by\n",
      "            hybdatadict with the parameters sd params'''\n",
      "            return hash_hyb_SD\n",
      "        @func_cache\n",
      "        def run_SD_oneparamfamily(param2vary,*paramrange,**defaultSDparams, **hybdatadict):\n",
      "        ''' Parameter to vary and hybrid dataset.\n",
      "         param2vary is a particular user adjustable variable in Spikedetekt\n",
      "          e.g. threshold_weak_std_factor \n",
      "            this function will loop over the values for param2vary over paramrange\n",
      "        (a one parameter family) and call run_spikedetekt()\n",
      "         with the defaultSDparams supplemented with the required \n",
      "           value for param2vary'''\n",
      "            return (all the hashes for each member of the family)\n",
      "        \n",
      "    detection_statistics.py: \n",
      "        import hash_utils.py\n",
      "        def test_detection_algorithm(**hybdatadict, **SDparams,**creation_groundtruth, **detectioncrit):\n",
      "             '''\n",
      "           creation_groundtruth consists of the equivalent of: \n",
      "         GroundtruthResfile,GroundtruthClufile,... (i.e. the times and the cluster labels for the\n",
      "         added hybrid spikes.\n",
      "      detection criteria include: \n",
      "          allowed_discrepancy,CSthreshold\n",
      "           This function will call SpikeSimilarityMeasure(a,b)\n",
      "             and output the file: Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth).kwik  \n",
      "           where detcrit_groundtruth is the groundtruth relative to the criteria, **detectioncrit '''\n",
      "            return detcrit_groundtruth\n",
      "        \n",
      "        def SpikeSimilarityMeasure(a,b):\n",
      "        ''' Computes spike similarity measure between masks a and b  CS(a,b) = a.b/|a||b|'''\n",
      "            return SSmeasure        \n",
      "\n",
      "    runklustakwik_lib.py:\n",
      "        # A collection of functions for running KlustaKwik\n",
      "        import hash_utils.py\n",
      "        def make_KK_inputdict(kkparams_tuple):\n",
      "        '''Will turn a tuple consisting of the list of common setting for Masked KlustaKwik\n",
      "         and will convert the output to a dictionary, e.g. PenaltyK, MinClusters, etc'''\n",
      "           return kkparams # a dictionary\n",
      "\n",
      "        @func_cache\n",
      "        def run_klustakwik(**hybdatadict,**sdprms,**kkparams, detcrit_groundtruth, script = True):\n",
      "         ''' will run KlustaKwik and produce a runscript for it, \n",
      "            e.g. a bash script for the supercomputer. \n",
      "             It will also specify a file system for the input and output files\n",
      "           by calling \n",
      "            hashclustername = hashutils.get_KK_output_hash(**hybdatadict,**sdprms,**kkparams)\n",
      "            If script = True, it will  produce a script like (useful for supercomputer,\n",
      "             where jobs must be submitted via a bash script:\n",
      "            script_pen=('/martinottihome/skadir/GIT_masters/klustakwik/MaskedKlustaKwik '\n",
      "                  +hybridsubsetfilebase+' 1 '\n",
      "                  '-MaskStarts 50 '\n",
      "                  '-MaxPossibleClusters 500 '\n",
      "                  '-FullStepEvery   1 '\n",
      "                  '-MaxIter 10000 '\n",
      "                  '-RandomSeed 654 '\n",
      "                  '-Debug 0 '\n",
      "                  '-SplitFirst 20 '\n",
      "                  '-SplitEvery 40 '\n",
      "                  '-PenaltyK  %g '\n",
      "                  '-PenaltyKLogN 0 '\n",
      "                  '-Subset  1 '\n",
      "                  '-PriorPoint 1 '\n",
      "                  '-SaveSorted 0 '        \n",
      "                  '-SaveCovarianceMeans 0 '\n",
      "                  '-UseMaskedInitialConditions 1 '\n",
      "                  '-AssignToFirstClosestMask 1 '\n",
      "                  '-UseDistributional 1 ') %(penal)\n",
      "            It will append Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_Hash(kkdefaultparams).kwik\n",
      "            output labelled by Hash(kkparams) in\n",
      "            Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_Hash(kkdefaultparams).kwik'''\n",
      "            return clustering_output #This will be stored in cache for analysis\n",
      "        def run_KK_oneparamfamily(arg2vary,argrange,**hybdatadict,**sdprms,detcrit_groundtruth, **kkdefaultparams):\n",
      "        ''' Will run KlustaKwik for a one parameter, arg2vary, family of values in argrange.\n",
      "            e.g. arg2vary = PenaltyK\n",
      "            It will first produce the file Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_KK_Hash(kkparams).kwik\n",
      "            which will store the len(argrange) clusterings\n",
      "            then it will run klustakwik on each value in argrange by calling\n",
      "            run_klustakwik(**hybdatadict,**sdprms,**kkparams) \n",
      "            where kkparams is kkdefaultparams with the appropriate arguments appended\n",
      "            Of course this can just be used for a single clustering\n",
      "                        '''\n",
      "    runsupervised_lib.py:\n",
      "        import hash_utils.py\n",
      "        @func_cache\n",
      "        def write_kwik(confusion_test,confusion_train):\n",
      "        '''writes cluster assignment output that can be read by klustaviewa\n",
      "            e.g. equivalent to 4 seasons .clu file'''\n",
      "            return seasonsclufileequivalent\n",
      "        def scale_data(**hybdatadict,**sdprms,detcrit_groundtruth ):\n",
      "        ''' Scales data between 0 and 1.\n",
      "            \n",
      "            Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth).kwx\n",
      "            Can use sklearn.preprocessing.scale\n",
      "            ''' \n",
      "            return scaled_data\n",
      "        @func_cache\n",
      "        def compute_grid_weights(gridweightparams):\n",
      "        ''' computes grid'''\n",
      "            return grid_weights\n",
      "        @func_cache\n",
      "        def compute_grid_C(gridparams):\n",
      "        ''' computes grid of C'''\n",
      "            return grid_C \n",
      "        def do_cross_validation_every(k_times):\n",
      "        ''' subdivide dataset into k_times parts'''\n",
      "        @func_cache\n",
      "        def learn_data(C,kernel, weights, detcrit_groundtruth, scaled_data, subsetvector = full):\n",
      "        '''utilises sklearn.svm.SVC, support vector machine, performs on\n",
      "           all features unless subsetvector is specified.'''\n",
      "            return confusion_test, confusion_train, clustering\n",
      "        @func_cache \n",
      "        def compute_errors(confusion_test,confusion_train):\n",
      "        '''computes errors from confusion matrices\n",
      "            True positive and false discovery rates will be computed'''\n",
      "            return errors\n",
      "        @func_cache \n",
      "        def learn_data_grid(grid_C,kernel, grid_weights,hybdatadict, sdparams,detcrit_groundtruth):\n",
      "        '''Write output as a clustering in \n",
      "           Hash(hybdatadict)_Hash(sdparams)_Hash(detcrit_groundtruth)_SVM_Hash(svmparams).kwik\n",
      "           It scales the data using scale_data() and \n",
      "           calls learn_data() for various values of the\n",
      "          grids and also the function compute_errors()'''\n",
      "            return error_grid\n",
      "           \n",
      "        \n",
      "            \n",
      "\n",
      "        \n",
      "    clustering_performance.py:\n",
      "        import hash_utils.py\n",
      "        def test_spike_sorting_algorithm(**hybdatadict,**KKparams, groundtruth_detcrit):\n",
      "        ''' will call create_confusion_matrix(), VImetric, EntropyH(), MutualInf()'''\n",
      "        def create_confusion_matrix():\n",
      "        ''' will create the confusion matrix, using the equivalent to a clu file\n",
      "          and groundtruth res and clu files'''\n",
      "        def VImetric(confusion_matrix):\n",
      "        '''Computes Meila's VI metric from the confusion matrix'''\n",
      "            return VImetric\n",
      "        def EntropyH():\n",
      "        ''' Required for computing Meila's VI metric '''\n",
      "            return entropyH\n",
      "        def MutualInf():\n",
      "            return mutual_inf\n",
      "        def compute_errors(confusion_matrix):\n",
      "        '''computes errors from confusion matrices\n",
      "            True positive and false discovery rates will be computed'''\n",
      "            return errors\n",
      "\n",
      "        \n",
      "    master_analysis_script.py: \n",
      "        #This will make use of functions in clustering_performance.py\n",
      "        \n",
      "        \n",
      "##Master scripts\n",
      "This will take the form:\n",
      "\n",
      "    import joblib (implicit in the other modules it calls)\n",
      "    import listofcreationdicts as cr\n",
      "    import listofprobes as pr\n",
      "    import listofSDparams as sd\n",
      "    import listofMKKparams as kk\n",
      "    # import mastercreationscript as mc\n",
      "   \n",
      "    import hybridata_creation_lib as hcl\n",
      "    import runspikedetekt_lib as rsd\n",
      "    import detection_statistics as dsa\n",
      "    import runklustakwik_lib as rkkl\n",
      "    import runsupervised_lib as rsl\n",
      "    \n",
      "    #I shall only list main operations\n",
      "    \n",
      "    predatasets = [cr.hybdataset1,cr.hybdataset2,cr.hybdataset3]\n",
      "    # These are dictionaries giving all the necessary information \n",
      "    # for the creation of hybrid datasets\n",
      "    \n",
      "    for predata in predatasets:\n",
      "        hcl.create_hybrid_kwdfile((**predata)\n",
      "    # create hybrid datasets \n",
      "    \n",
      "    datasets = [cr.hybdataset1,cr.hybdataset2,cr.hybdataset3]  \n",
      "    # repetition only for emphasis and to avoid bugs\n",
      "    \n",
      "    for hybdatadict in datasets: \n",
      "        rsd.run_SD_oneparamfamily(param2vary,*paramrange,**defaultSDparams, **hybdatadict)\n",
      "    # runs spikedetekt several times on each dataset \n",
      "    \n",
      "    detectioncrit = [time_window, CSmin]\n",
      "    SDoutputlist = [(cr.hybdatadict1, sd.SDparams)]\n",
      "    \n",
      "    for (hybdatadict, SDparams) in SDoutputlist:\n",
      "        detcrit_groundtruth = dsa.test_detection_algorithm(**hybdatadict, **SDparams,**groundtruth, **detectioncrit)\n",
      "    #tests detection\n",
      "    \n",
      "    penalty_range = [0, 0.1, 0.01, 1, 2, 4]\n",
      "    \n",
      "    for hybdatadict in datasets: \n",
      "        run_KK_oneparamfamily(penalty2vary,penaltyrange,**hybdatadict,**sdprms,detcrit_groundtruth, **kk.kkdefaultparams)\n",
      "        \n",
      "    #runs KlustaKwik\n",
      "    \n",
      "    for hybdatadict in datasets: \n",
      "        rsl.learn_data(C,kernel, weights, groundtruth_detcrit, hybdatadict)\n",
      "        rsl.compute_errors(confusion_test,confusion_train)\n",
      "    #runs supervised learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}