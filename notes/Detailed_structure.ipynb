{
 "metadata": {
  "name": "Detailed_structure"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Folder structure\n",
      "In summary the folder structure will look as follows:\n",
      "\n",
      "    /Creation\n",
      "        mastercreationscript.py\n",
      "        #One script that will run though the creation of a set of hybridatasets\n",
      "        \n",
      "        hybridata_creation_lib.py\n",
      "        # Will contain all the functions for creating hybrid datasets\n",
      "        /Mariano\n",
      "            /donors\n",
      "            # Contains donor .dat files \n",
      "            /timeseries\n",
      "            # Contains input .res files pertaining to how to add spikes\n",
      "            #and donor derived files such as: \n",
      "            #meanspike_file_id = a file called donor_id.msua.1.\n",
      "            #meanmask_file = a file called donor_id.amsk.1.\n",
      "            /acceptors \n",
      "            # contains acceptor .dat files\n",
      "    \n",
      "    \n",
      "    /Testsuite\n",
      "        listofdicts.py  # Here we shall store all the tuples that we need to create\n",
      "        #and later access hybrid datasets. \n",
      "        \n",
      "        listofprobes.py # Give dictionaries pertaining to different probes\n",
      "        \n",
      "        listofSDparams.py #Here we need a file containing many different tuples and dictionaries\n",
      "        #all pertaining to different ways of running SpikeDetekt.\n",
      "        # It should certainly contain all the **defaultSDparams, when I run\n",
      "        # SpikeDetekt several times varying one parameter using the function\n",
      "        # run_SD_oneparamfamily(*param2vary,**defaultSDparams, **hybdatadict)\n",
      "        \n",
      "        listofMKKparams.py\n",
      "        # Here we shall have a file storing dictionaries which have key, value\n",
      "        #pairs which specify\n",
      "        \n",
      "\n",
      "        \n",
      "        runspikedetekt_lib.py\n",
      "        # Will contain all functions for running spikedetekt on hybrid datasets\n",
      "        \n",
      "        detection_statistics.py\n",
      "        #Will containt all functions for analysing the performance of spikedetekt\n",
      "        #and for producing the groundtruth relative to which the performance \n",
      "        # of unsupervised spike sorting can be tested.\n",
      "        \n",
      "        runklustakwik_lib.py\n",
      "        #Will contain all functions for running KlustaKwik given a\n",
      "        #hybrid dataset, a detection with groundtruth by spikedetekt\n",
      "        \n",
      "        runsupervised_lib.py\n",
      "        #Will contain all functions for running sklearn SVM given a\n",
      "        #hybrid dataset, a detection with groundtruth by spikedetekt\n",
      "        # we will compare two kernels: quadric and Gaussian radial \n",
      "        #basis kernel.\n",
      "        \n",
      "        clustering_performance.py\n",
      "        # Will contain all functions necessary for evaluating the performance of \n",
      "        #KlustaKwik and SVM. All of these will be decorated with joblib.\n",
      "        # Graphs can be plotted with matplotlib - .e.g. ROC curves.\n",
      "        \n",
      "        \n",
      "\n",
      " \n",
      "        /Hybrids\n",
      "      \n",
      "            Hash(hybdatadict).kwd\n",
      "            Hash(hybdatadict)_Hash(sdparams).kwx\n",
      "            Hash(hybdatadict)_Hash(sdparams).kwik  \n",
      "            Hash(hybdatadict)_Hash(sdparams)_Sigma.kwik   \n",
      "            Hash(hybdatadict)_Hash(sdparams)_Sigma_Hash(kkparams).kwik\n",
      "            Hash(hybdatadict)_Hash(sdparams)_Sigma_Hash(svmparams).kwik\n",
      "        /Analysis\n",
      "            analysis_cache\n",
      "            \n",
      "#Detailed specification of files, functions and classes\n",
      "            \n",
      "Each script will contain the following functions and classes:\n",
      "    \n",
      "    listofcreationtuples.py:\n",
      "        donorlist = [donor,donorcluid,donorcluster]\n",
      "        acceptorlist = [acceptor]\n",
      "        time_size_list = [amplitude, time_series]\n",
      "        #hybdatarglist = [acceptor, donor_id, amplitude, time_series]\n",
      "        \n",
      "    listofdicts.py:    \n",
      "        hybdatadict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54,\n",
      "                   'acceptor':'n6mab041109quarter1', 'amplitude':0.5,\n",
      "                   'donorspike_timeseries': 'regular2p5'}\n",
      "        \n",
      "    listofSDparams.py:  \n",
      "        SDusualparams # dictionaries of default parameters used for different\n",
      "        #testings of SD\n",
      "    \n",
      "    listofKKparams.py: \n",
      "        kkdefaultparams\n",
      "        #Dictionary of KK comand line options\n",
      "    \n",
      "    input_generator.py: (will output to listoftuples.py)\n",
      "        def convert_tuple_to_dict(*arglist):\n",
      "        ''' Will convert my list of tuples into dictionaries of the same length\n",
      "         adding the appropriate keys, e.g.\n",
      "          donordata = (donor,donorcluid,donorcluster) will turn into a dictionary like:\n",
      "         donordict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54} '''    \n",
      "        def merge_inputdicts(donordict,acceptordict,time_size_dict): \n",
      "        ''' Will merge all the necessary tuples which point to the files needed to \n",
      "        create a hybrid dataset'''\n",
      "            return hybdatadict    \n",
      "        \n",
      "    hybridata_creation.py:\n",
      "    #A collection of functions for creating hybrid datasets\n",
      "        def create_donor_id(): \n",
      "        '''Outputs donor identity files: \n",
      "          donordata = (donor,donorcluid,donorcluster)      \n",
      "          donor_id = donor_donorcluid_donorcluster, (which typically looks like: n6mab031109_MKKdistfloat_54`).\n",
      "          will call function: create_average_hybrid_wave_spike(**donor_dict)\n",
      "          meanspike_file_id = a file called donor_id.msua.1.\n",
      "          Will call make_average datamask_from_mean(**donor_dict)\n",
      "          meanmask_file = a file called donor_id.amsk.1.\n",
      "          It will return a dictionary: \n",
      "           donor_dict = {'donor': 'n6mab031109', 'donorcluid': 'MKKdistfloat', 'donorcluster': 54 }\n",
      "           This function should be decorated. '''\n",
      "        def create_average_hybrid_wave_spike(**donor_dict):\n",
      "        '''will call function: create_average_hybrid_wave_spike(**donor_dict)\n",
      "          meanspike_file_id = a file called donor_id.msua.1.''''\n",
      "        def make_average datamask_from_mean(**donor_dict):\n",
      "        '''Will call make_average datamask_from_mean(**donor_dict)\n",
      "          meanmask_file = a file called donor_id.amsk.1.'''\n",
      "        def create_hybdatfile_directory_namehash(**hybdatadict): \n",
      "            @func_cache\n",
      "        def create_hybrid_kwdfile(hybdatadict):\n",
      "            \n",
      "        '''This function outputs a raw datafile called, \n",
      "              Hash(D).kwd, in the folder Hash(D)which\n",
      "           will be produced via the hash of the input dictionary\n",
      "            '''\n",
      "        hashD = hash(sorted(hybdatadict.items(), key=lambda (key, val): key))\n",
      "        return hashD    \n",
      "         \n",
      "        \n",
      "    runspikedetekt_lib.py:\n",
      "        #A collection of functions for running SpikeDetekt\n",
      "        #All decorated\n",
      "        def hash_SD_hyb(sdprms,hybdatadict):\n",
      "        #OK, I realise that I can't use this notation, so I suppose I have to produce one dictionary\n",
      "        # containing both the SD parameters and the hybrid dataset parameters\n",
      "        def run_spikedetekt(sdprms,hybdatadict):\n",
      "        '''This function will call hash_SD_hyb(sdprms,hybdatadict) '''\n",
      "        def run_SD_oneparamfamily(*param2vary,**defaultSDparams, **hybdatadict):\n",
      "        ''' Parameter to vary and hybrid dataset.\n",
      "         **param2vary is a list of the form\n",
      "            this will loop\n",
      "        through a one parameter family and call run_spikedetekt()\n",
      "         with the defaultSDparams supplemented with the required \n",
      "          extra variable'''\n",
      "\n",
      "    runklustakwik_lib.py:\n",
      "        # A collection of functions for running KlustaKwik\n",
      "        def make_KK_inputdict(kkparams_tuple):\n",
      "        '''Will turn a tuple consisting of the list of common setting for Masked KlustaKwik\n",
      "         and will convert the output to a dictionary, e.g. PenaltyK, MinClusters, etc'''\n",
      "           return kkparams # a dictionary\n",
      "        def get_KK_directory(**hybdatadict,**sdprms,**kkparams):\n",
      "        '''gives the product hash of the three input dictionaries'''\n",
      "        \n",
      "        def run_klustakwik(**hybdatadict,**sdprms,**kkparams):\n",
      "         ''' will run KlustaKwik and produce a runscript for it, eg. a bash script for the \n",
      "           supercomputer. \n",
      "             It will also specify a file system for the input and output files\n",
      "           by calling \n",
      "            get_KK_directory(**hybdatadict,**sdprms,**kkparams)\n",
      "            Something like:\n",
      "            script_pen=('/martinottihome/skadir/GIT_masters/klustakwik/MaskedKlustaKwik '\n",
      "                  +hybridsubsetfilebase+' 1 '\n",
      "                #   '-MinClusters 4 '\n",
      "                #   '-MaxClusters 10 '\n",
      "                  '-MaskStarts 50 '\n",
      "                  #'-PenaltyMix 1.0 '\n",
      "                  '-MaxPossibleClusters 500 '\n",
      "                  '-FullStepEvery   1 '\n",
      "                  '-MaxIter 10000 '\n",
      "                  '-RandomSeed 654 '\n",
      "                  '-Debug 0 '\n",
      "                  '-SplitFirst 20 '\n",
      "                  '-SplitEvery 40 '\n",
      "                  '-PenaltyK  %g '\n",
      "                  '-PenaltyKLogN 0 '\n",
      "                  '-Subset  1 '\n",
      "                  '-PriorPoint 1 '\n",
      "                  '-SaveSorted 0 '        \n",
      "                  '-SaveCovarianceMeans 0 '\n",
      "                  '-UseMaskedInitialConditions 1 '\n",
      "                  '-AssignToFirstClosestMask 1 '\n",
      "                  #'-RandomSeed '+str(int(randint(100000)))+' '\n",
      "                  '-UseDistributional 1 ') %(penal)'''\n",
      "            return clustering_output #This will be stored in cache for analysis\n",
      "        def get_KK_directories(penalty2vary,penaltyrange,**hybdatadict,**sdprms,**kkdefaultparams):\n",
      "        ''' calls get_KK_directory(**hybdatadict,**sdprms,**kkparams)\n",
      "           for each penalty value and runs KK by calling\n",
      "            run_klustakwik(**hybdatadict,**sdprms,**kkparams) \n",
      "                        '''\n",
      "    runsupervised_lib.py:\n",
      "        def write_kwik(confusion_test,confusion_train):\n",
      "        '''writes cluster assignment output that can be read by klustaviewa\n",
      "            e.g. equivalent to 4 seasons .clu file'''\n",
      "        def select_subsetfeatures(feature_vector):\n",
      "        ''' If you want to only perform supervised learning on a subset of features'''\n",
      "        def scale_data():\n",
      "        ''' Scales data between 0 and 1. Can use sklearn.preprocessing.scale''' \n",
      "        def compute_grid_weights():\n",
      "        ''' computes grid'''\n",
      "            return grid_weights\n",
      "        def compute_grid_C():\n",
      "        ''' computes grid of C'''\n",
      "            return grid_C \n",
      "        def do_cross_validation_every(k_times):\n",
      "        ''' subdivide dataset into k_times parts'''\n",
      "        def learn_data(C,kernel, weights, groundtruth, data):\n",
      "        '''utilises sklearn.svm.SVC, support vector machine'''\n",
      "            return confusion_test, confusion_train\n",
      "        def learn_data_grid(grid_C,kernel, grid_weights, groundtruth, data):\n",
      "        ''' calls learn_data for various values of the\n",
      "          grids'''\n",
      "            returns a_grid_confusion_test,  a_grid_confusion_train\n",
      "        def compute_errors(confusion_test,confusion_train):\n",
      "        '''computes errors from confusion matrices'''\n",
      "            return .\n",
      "            \n",
      "    detection_statistics.py: \n",
      "        def test_detection_algorithm(**hybdatadict, **SDparams,**groundtruth, **detectioncrit):\n",
      "             '''\n",
      "           ground truth consists of the equivalent of: \n",
      "         GroundtruthResfile,GroundtruthClufile,...\n",
      "      detection criteria include: \n",
      "          allowed_discrepancy,CSthreshold'''\n",
      "        \n",
      "        def SpikeSimilarityMeasure(a,b):\n",
      "        ''' Computes spike similarity measure between masks a and b  CS(a,b) = a.b/|a||b|'''\n",
      "        \n",
      "    clustering_performance.py:\n",
      "        def: get_cache_hash_KK(**hybdatadict):\n",
      "        def test_spike_sorting_algorithm(**hybdatadict,**KKparams, groundtruth_detcrit):\n",
      "        ''' will call create_confusion_matrix(), VImetric, EntropyH(), MutualInf()'''\n",
      "        def create_confusion_matrix():\n",
      "        ''' will create the confusion matrix, using the equivalent to a clu file\n",
      "          and groundtruth res and clu files'''\n",
      "        def VImetric():\n",
      "        def EntropyH():\n",
      "        def MutualInf():\n",
      "        def compute_errors(confusion_matrix):\n",
      "        \n",
      "    master_analysis_script.py: \n",
      "        #This will make use of functions in clustering_performance.py\n",
      "        #and call upon the directory names via the function\n",
      "        seek_variables(**hybdatadict,**KKparams, groundtruth_detcrit):\n",
      "        \n",
      "##Master scripts\n",
      "This will take the form:\n",
      "\n",
      "    import joblib (implicit in the other modules it calls)\n",
      "    import listoftuples\n",
      "    import listofSDparams\n",
      "    import listofMKKparams\n",
      "    # import mastercreationscript as mc\n",
      "   \n",
      "    import hybridata_creation_lib as hcl\n",
      "    import runspikedetekt_lib as rsd\n",
      "    import detection_statistics as ds\n",
      "    import runklustakwik_lib\n",
      "    import runsupervised_lib.py\n",
      "    \n",
      "    predatasets = [prehybdataset1,prehybdataset2,prehybdataset3]\n",
      "    \n",
      "    for predata in predatasets:\n",
      "        hcl.create_hybrid_datfile(**predata)\n",
      "    #create hybrid datasets\n",
      "    \n",
      "    datasets = [hybdataset1,hybdataset2,hybdataset3]            \n",
      "    \n",
      "    for hybdatadict in datasets: \n",
      "        rsd.run_SD_oneparamfamily(*param2vary,**defaultsSDparams,**hybdatadict)\n",
      "    # run spikedetekt\n",
      "    \n",
      "    detectioncrit = [time_window, CSmin]\n",
      "    SDoutputlist = [(hybdatadict, SDparams)]\n",
      "    \n",
      "    for (hybdatadict, SDparams) in SDoutputlist\n",
      "        ds.test_detection_algorithm(**hybdatadict, **SDparams,**groundtruth, **detectioncrit)\n",
      "    #test detection\n",
      "    \n",
      "    penalty_range = [0, 0.1, 0.01, 1, 2, 4]\n",
      "    \n",
      "    for hybdatadict in datasets: \n",
      "        get_KK_directories(penalty2vary,penaltyrange,**hybdatadict,**sdprms,**kkdefaultparams)\n",
      "    #run KlustaKwik\n",
      "    \n",
      "    for hybdatadict in datasets: \n",
      "        learn_data(C,kernel, weights, groundtruth_detcrit, hybdatadict)\n",
      "        compute_errors(confusion_test,confusion_train)\n",
      "    #run supervised learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}